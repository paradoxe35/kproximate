# Kproximate Development Environment Configuration

# Debug mode
DEBUG=true

# Application configuration
POLL_INTERVAL=10
MAX_KP_NODES=3
LOAD_HEADROOM=0.2
WAIT_SECONDS_FOR_JOIN=120
WAIT_SECONDS_FOR_PROVISION=120

# Scale-down stabilization configuration
# Prevents scale-down if any node was created within this period (in minutes)
# This helps prevent infinite scale-up/scale-down cycles
SCALE_DOWN_STABILIZATION_MINUTES=5

# Minimum age in minutes before a node can be considered for scale-down
# This prevents newly created nodes from being immediately removed
MIN_NODE_AGE_MINUTES=10

# RabbitMQ configuration
RABBITMQ_CONTAINER_NAME=kproximate-rabbitmq
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest
RABBITMQ_PORT=5672
RABBITMQ_MANAGEMENT_PORT=15672
# Set to false for local development (no TLS)
RABBITMQ_USE_TLS=false

# Proxmox configuration
# Required for full functionality
PM_URL=https://datacenter.mydomain.net:8006/api2/json
PM_USER_ID=kproximate@pam!kproximate
# Use either PM_PASSWORD or PM_TOKEN
PM_PASSWORD=your-password
PM_TOKEN=42adb612-906d-425e-a46a-aff2826162e2
PM_ALLOW_INSECURE=false
PM_DEBUG=false

# Node configuration
KP_NODE_TEMPLATE_NAME=ubuntu-22.04-cloudinit-template
KP_NODE_NAME_PREFIX=kp-node
KP_NODE_CORES=4
KP_NODE_MEMORY=4096
KP_NODE_DISABLE_SSH=false
KP_QEMU_EXEC_JOIN=true
KP_LOCAL_TEMPLATE_STORAGE=true

# Additional configuration
KP_NODE_LABELS=topology.kubernetes.io/region=proxmox-cluster,topology.kubernetes.io/zone={{ .TargetHost }}
# SSH_KEY=your-ssh-key

# Node Selection Strategy Configuration
# Strategy for selecting Proxmox hosts when scaling up
# Options: spread, max-memory, max-cpu, balanced, round-robin
NODE_SELECTION_STRATEGY=spread

# Minimum resource requirements for host eligibility
# Hosts that don't meet these requirements will be excluded from selection
MIN_AVAILABLE_CPU_CORES=0
MIN_AVAILABLE_MEMORY_MB=0

# Comma-separated list of Proxmox node names to exclude from scaling up
# Useful for maintenance, dedicated workloads, or resource constraints
EXCLUDED_NODES=""

# Enhanced Autoscaling Configuration
# These features provide proactive scaling based on resource pressure, scheduling errors, and storage constraints

# Resource Pressure Thresholds
# Enable proactive scaling based on CPU and memory utilization before pods become unschedulable
ENABLE_RESOURCE_PRESSURE_SCALING=true

# CPU utilization threshold (0.0-1.0). Triggers scaling when cluster CPU usage exceeds this percentage.
# Recommended: 0.7-0.8 for production workloads, 0.8-0.85 for development
CPU_UTILIZATION_THRESHOLD=0.8

# Memory utilization threshold (0.0-1.0). Triggers scaling when cluster memory usage exceeds this percentage.
# Recommended: 0.8-0.85 for production workloads, 0.85-0.9 for development
MEMORY_UTILIZATION_THRESHOLD=0.8

# Pod Scheduling Error Events
# Enable scaling based on pod scheduling failures to quickly respond to resource constraints
ENABLE_SCHEDULING_ERROR_SCALING=true

# Number of unique failed pods to trigger scaling. Monitors scheduling failures in the last 5 minutes.
# Recommended: 3-5 for most environments, 2-3 for strict SLA environments, 5-10 for batch workloads
SCHEDULING_ERROR_THRESHOLD=3

# Storage Pressure Configuration
# Enable scaling based on disk space utilization to prevent node failures due to disk exhaustion
ENABLE_STORAGE_PRESSURE_SCALING=true

# Disk utilization threshold (0.0-1.0). Triggers scaling when any node's disk usage exceeds this percentage.
# Recommended: 0.85-0.9 for production, 0.8-0.85 for strict environments
DISK_UTILIZATION_THRESHOLD=0.85

# Minimum available disk space in GB. Triggers scaling when any node has less than this amount of free space.
# Recommended: 5-10 GB depending on workload, 10-20 GB for data-intensive applications
MIN_AVAILABLE_DISK_SPACE_GB=5

# Configuration Examples for Different Scenarios:
#
# Development Environment (Conservative):
# ENABLE_RESOURCE_PRESSURE_SCALING=false
# ENABLE_SCHEDULING_ERROR_SCALING=false
# ENABLE_STORAGE_PRESSURE_SCALING=false
#
# Production with Strict SLAs (Aggressive):
# CPU_UTILIZATION_THRESHOLD=0.70
# MEMORY_UTILIZATION_THRESHOLD=0.75
# SCHEDULING_ERROR_THRESHOLD=2
# DISK_UTILIZATION_THRESHOLD=0.80
# MIN_AVAILABLE_DISK_SPACE_GB=10
#
# High-Performance Computing Workloads:
# CPU_UTILIZATION_THRESHOLD=0.65
# MEMORY_UTILIZATION_THRESHOLD=0.85
# SCHEDULING_ERROR_THRESHOLD=2
#
# Batch Processing Workloads (Higher tolerance):
# CPU_UTILIZATION_THRESHOLD=0.85
# MEMORY_UTILIZATION_THRESHOLD=0.90
# SCHEDULING_ERROR_THRESHOLD=5

# Kubernetes join command
# This is the command that will be executed on the new node to join the Kubernetes cluster
KP_JOIN_COMMAND=""

# Kubernetes configuration
# Path to kubeconfig file for accessing the Kubernetes cluster
# KUBECONFIG=/path/to/kubeconfig
